{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FqN6vt5hjJxq"
      },
      "outputs": [],
      "source": [
        "# required libraries\n",
        "!pip install transformers datasets --quiet\n",
        "!pip install matplotlib rouge-score --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "!pip install accelerate -U --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MxOKb_pOjReh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric, DatasetDict, load_from_disk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30BB8KS5jRbR"
      },
      "outputs": [],
      "source": [
        "# drive to store the trained model and datasets\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpO0w3YQjRZL"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available and set the device accordingly\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRXki2OqvEeI"
      },
      "outputs": [],
      "source": [
        "# Set up model\n",
        "model_name = 'google/pegasus-cnn_dailymail'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TAcXxlozjRUc"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"article\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Load and tokenize the dataset\n",
        "tokenized_dataset_path = '/content/drive/MyDrive/Pegasus/tokenized_datasets'  # Adjust the path as needed\n",
        "if os.path.exists(tokenized_dataset_path):\n",
        "    tokenized_datasets = DatasetDict.load_from_disk(tokenized_dataset_path)\n",
        "else:\n",
        "    # Load dataset\n",
        "    dataset_path = '/content/drive/MyDrive/Pegasus/dataset'\n",
        "    if os.path.exists(dataset_path):\n",
        "      dataset = load_from_disk(dataset_path)\n",
        "    else:\n",
        "      dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "      dataset.save_to_disk(dataset_path)\n",
        "    dataset['train'] = dataset['train'].select(range(10000))\n",
        "    dataset['validation'] = dataset['validation'].select(range(2000))\n",
        "    dataset['test'] = dataset['test'].select(range(100))\n",
        "    # Tokenize dataset\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets.save_to_disk(tokenized_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDqBkVl4jRPe"
      },
      "outputs": [],
      "source": [
        "# hyper parameters for the fine-tuning of the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"logs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x21tJpHLjRKu"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer which trains and validates the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5cY4z4fjRH_"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdQxi0xGl-9y"
      },
      "outputs": [],
      "source": [
        "# path to save the models and hyperparameters\n",
        "model_path = '/content/drive/MyDrive/Pegasus/model'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "_s9Df1KZAQMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyEuXN9pzSon"
      },
      "outputs": [],
      "source": [
        "# saving the tokenizer\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5r_kdrfxsgM"
      },
      "outputs": [],
      "source": [
        "# saving the hyper parameters to continue fine-tuning\n",
        "training_args_dict = training_args.to_dict()\n",
        "with open('/content/drive/MyDrive/Pegasus/training_args.json', 'w') as f:\n",
        "    json.dump(training_args_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-9C3k9lNyBmx"
      },
      "outputs": [],
      "source": [
        "# loading the saved parameters\n",
        "with open('/content/drive/MyDrive/Pegasus/training_args.json', 'r') as f:\n",
        "    loaded_args_dict = json.load(f)\n",
        "loaded_training_args = TrainingArguments(**loaded_args_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UhEC5urBmBt8"
      },
      "outputs": [],
      "source": [
        "# loading the saved tokenizer and model\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_path)\n",
        "fine_tuned_model = PegasusForConditionalGeneration.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cZYBrHX9BccU"
      },
      "outputs": [],
      "source": [
        "# loading the tokenized test dataset for ROGUE score calculation\n",
        "test_dataset = tokenized_datasets['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "109ujE0rBbxw"
      },
      "outputs": [],
      "source": [
        "# generate preidcted summaries for each article in the test dataset\n",
        "def generate_predictions(batch):\n",
        "    input_ids = torch.tensor(batch['input_ids'])\n",
        "    outputs = fine_tuned_model.generate(input_ids, max_length=125, num_beams=5, early_stopping=True)\n",
        "    batch['predicted_summary'] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return batch\n",
        "\n",
        "result = test_dataset.map(generate_predictions, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0gKmRMWBbt6"
      },
      "outputs": [],
      "source": [
        "rouge = load_metric('rouge')\n",
        "\n",
        "def compute_rouge_scores(outputs, references):\n",
        "    return rouge.compute(predictions=outputs, references=references, use_aggregator=True, use_stemmer=True)\n",
        "\n",
        "# Extract the summaries and the references\n",
        "decoded_preds = [prediction['predicted_summary'] for prediction in result]\n",
        "decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in test_dataset['labels']]\n",
        "\n",
        "# Compute the scores\n",
        "rouge_scores = compute_rouge_scores(decoded_preds, decoded_labels)\n",
        "\n",
        "# Calculate average scores\n",
        "average_scores = {}\n",
        "for key in rouge_scores:\n",
        "    score = rouge_scores[key]\n",
        "    average_scores[key] = {\n",
        "        'precision': score.mid.precision * 100,\n",
        "        'recall': score.mid.recall * 100,\n",
        "        'fmeasure': score.mid.fmeasure * 100\n",
        "    }\n",
        "print(average_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj_8s_V1jQ-g"
      },
      "outputs": [],
      "source": [
        "# Example text to summarize\n",
        "# manual testing with inputs\n",
        "text_to_summarize = input(\"\")\n",
        "\n",
        "# Encode the text and generate summary\n",
        "inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", truncation=True)\n",
        "summary_ids = model.generate(inputs, max_length=250, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'summary:{summary}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ5TA_GTBboC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYwMdbpyBblE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf9JIvS1BbiH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBfEns0sBbe5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L-WsWIFBbbw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}